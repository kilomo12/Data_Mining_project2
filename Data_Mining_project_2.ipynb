{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project 2 - Complete Analysis\n",
    "\n",
    "This notebook contains a full end-to-end analysis for the `cmi_internet.csv` dataset:\n",
    "- data loading and data quality checks\n",
    "- exploratory analysis (missing values, class imbalance)\n",
    "- preprocessing pipeline\n",
    "- baseline classification models\n",
    "- imbalance handling strategies\n",
    "- final comparison and conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "We use a relative path first, with a fallback to your absolute local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Try project-relative path first\n",
    "relative_path = Path('dm2_25_26_dataset_tabular/DM2_project/cmi_internet.csv')\n",
    "absolute_fallback = Path('C:/Users/steve/Downloads/Data_Mining_project2/dm2_25_26_dataset_tabular/DM2_project/cmi_internet.csv')\n",
    "\n",
    "if relative_path.exists():\n",
    "    data_path = relative_path\n",
    "elif absolute_fallback.exists():\n",
    "    data_path = absolute_fallback\n",
    "else:\n",
    "    raise FileNotFoundError('Dataset file not found in expected locations.')\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f'Data path: {data_path}')\n",
    "print(f'Shape: {df.shape}')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview and Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# General information about the dataset\n",
    "display(df.info())\n",
    "display(df.describe(include='all').T.head(10))\n",
    "\n",
    "# Identify target and remove obvious identifier from modeling\n",
    "target_col = 'sii'\n",
    "id_col = 'id' if 'id' in df.columns else None\n",
    "\n",
    "print(f'Target column: {target_col}')\n",
    "print(f'ID column: {id_col}')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "missing_count = df.isna().sum()\n",
    "missing_pct = (missing_count / len(df) * 100).sort_values(ascending=False)\n",
    "\n",
    "print('Number of columns with at least one missing value:', (missing_count > 0).sum())\n",
    "print()\n",
    "print('Top 20 columns by missing percentage:')\n",
    "display(missing_pct.head(20).to_frame('missing_%'))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target distribution (class imbalance check)\n",
    "class_counts = df[target_col].value_counts().sort_index()\n",
    "class_pct = (class_counts / class_counts.sum() * 100).round(2)\n",
    "\n",
    "print('Target counts:')\n",
    "print(class_counts)\n",
    "print()\n",
    "print('Target percentages (%):')\n",
    "print(class_pct)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "sns.barplot(x=class_counts.index.astype(int), y=class_counts.values, ax=ax[0], palette='viridis')\n",
    "ax[0].set_title('Target Class Counts (sii)')\n",
    "ax[0].set_xlabel('Class')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "ax[1].pie(class_counts.values, labels=class_counts.index.astype(int), autopct='%1.1f%%', startangle=90)\n",
    "ax[1].set_title('Target Class Distribution (sii)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize top missing-value columns\n",
    "top_missing = missing_pct.head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_missing.values, y=top_missing.index, palette='magma')\n",
    "plt.title('Top 15 Columns by Missing Percentage')\n",
    "plt.xlabel('Missing %')\n",
    "plt.ylabel('Column')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split and Preprocessing\n",
    "\n",
    "We build a robust preprocessing pipeline:\n",
    "- numeric features: median imputation + scaling\n",
    "- categorical features: most frequent imputation + one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop(columns=[target_col] + ([id_col] if id_col else []), errors='ignore')\n",
    "y = df[target_col].astype(int)\n",
    "\n",
    "# Stratified split preserves class proportions in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "print('Numeric features:', len(numeric_features))\n",
    "print('Categorical features:', len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preprocessing blocks\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Evaluation helper\n",
    "\n",
    "def evaluate_pipeline(model_name, estimator, X_tr, y_tr, X_te, y_te):\n",
    "    pipe = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('model', estimator)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_pred = pipe.predict(X_te)\n",
    "\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy_score(y_te, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_te, y_pred),\n",
    "        'macro_f1': f1_score(y_te, y_pred, average='macro'),\n",
    "        'weighted_f1': f1_score(y_te, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    return pipe, y_pred, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "We compare simple and stronger baselines:\n",
    "- `DummyClassifier` (reference baseline)\n",
    "- `LogisticRegression`\n",
    "- `DecisionTreeClassifier`\n",
    "- `RandomForestClassifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_models = {\n",
    "    'dummy_prior': DummyClassifier(strategy='prior', random_state=42),\n",
    "    'logistic_regression': LogisticRegression(max_iter=1000),\n",
    "    'decision_tree': DecisionTreeClassifier(random_state=42, min_samples_leaf=10),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "baseline_results = []\n",
    "fitted_baselines = {}\n",
    "preds_baselines = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    fitted_pipe, y_pred, metrics = evaluate_pipeline(name, model, X_train, y_train, X_test, y_test)\n",
    "    baseline_results.append(metrics)\n",
    "    fitted_baselines[name] = fitted_pipe\n",
    "    preds_baselines[name] = y_pred\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values('macro_f1', ascending=False)\n",
    "display(baseline_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Detailed classification reports for baseline models\n",
    "for name in baseline_df['model']:\n",
    "    print('=' * 90)\n",
    "    print(f'Classification report: {name}')\n",
    "    print(classification_report(y_test, preds_baselines[name], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion matrices for selected baseline models\n",
    "selected_baselines = ['dummy_prior', 'decision_tree', 'random_forest']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(selected_baselines), figsize=(18, 5))\n",
    "if len(selected_baselines) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "labels = sorted(y.unique())\n",
    "for ax, model_name in zip(axes, selected_baselines):\n",
    "    cm = confusion_matrix(y_test, preds_baselines[model_name], labels=labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imbalance Handling Strategies\n",
    "\n",
    "Since `sii` is clearly imbalanced, we test:\n",
    "1. algorithm-level balancing using `class_weight`\n",
    "2. data-level random oversampling (training set only)\n",
    "3. data-level random undersampling (training set only)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Weighted models (algorithm-level balancing)\n",
    "weighted_models = {\n",
    "    'logreg_weighted': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'dtree_weighted': DecisionTreeClassifier(random_state=42, min_samples_leaf=10, class_weight='balanced'),\n",
    "    'rf_weighted': RandomForestClassifier(\n",
    "        n_estimators=250, random_state=42, n_jobs=-1, class_weight='balanced_subsample'\n",
    "    )\n",
    "}\n",
    "\n",
    "weighted_results = []\n",
    "fitted_weighted = {}\n",
    "preds_weighted = {}\n",
    "\n",
    "for name, model in weighted_models.items():\n",
    "    fitted_pipe, y_pred, metrics = evaluate_pipeline(name, model, X_train, y_train, X_test, y_test)\n",
    "    weighted_results.append(metrics)\n",
    "    fitted_weighted[name] = fitted_pipe\n",
    "    preds_weighted[name] = y_pred\n",
    "\n",
    "weighted_df = pd.DataFrame(weighted_results).sort_values('macro_f1', ascending=False)\n",
    "display(weighted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Manual random over/under-sampling helpers (no external package required)\n",
    "def random_oversample(X_in, y_in, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data = X_in.copy()\n",
    "    data['_target_'] = y_in.values\n",
    "\n",
    "    counts = data['_target_'].value_counts()\n",
    "    max_count = counts.max()\n",
    "\n",
    "    sampled_parts = []\n",
    "    for cls, cls_count in counts.items():\n",
    "        part = data[data['_target_'] == cls]\n",
    "        if cls_count < max_count:\n",
    "            # Sample with replacement to match majority class size\n",
    "            extra_idx = rng.choice(part.index.to_numpy(), size=max_count - cls_count, replace=True)\n",
    "            part = pd.concat([part, part.loc[extra_idx]], axis=0)\n",
    "        sampled_parts.append(part)\n",
    "\n",
    "    out = pd.concat(sampled_parts, axis=0).sample(frac=1.0, random_state=random_state)\n",
    "    return out.drop(columns=['_target_']), out['_target_'].astype(int)\n",
    "\n",
    "\n",
    "def random_undersample(X_in, y_in, random_state=42):\n",
    "    data = X_in.copy()\n",
    "    data['_target_'] = y_in.values\n",
    "\n",
    "    counts = data['_target_'].value_counts()\n",
    "    min_count = counts.min()\n",
    "\n",
    "    sampled_parts = []\n",
    "    for cls in counts.index:\n",
    "        part = data[data['_target_'] == cls].sample(n=min_count, random_state=random_state, replace=False)\n",
    "        sampled_parts.append(part)\n",
    "\n",
    "    out = pd.concat(sampled_parts, axis=0).sample(frac=1.0, random_state=random_state)\n",
    "    return out.drop(columns=['_target_']), out['_target_'].astype(int)\n",
    "\n",
    "\n",
    "X_train_over, y_train_over = random_oversample(X_train, y_train)\n",
    "X_train_under, y_train_under = random_undersample(X_train, y_train)\n",
    "\n",
    "print('Original train distribution:')\n",
    "print(y_train.value_counts().sort_index())\n",
    "print()\n",
    "print('Oversampled train distribution:')\n",
    "print(y_train_over.value_counts().sort_index())\n",
    "print()\n",
    "print('Undersampled train distribution:')\n",
    "print(y_train_under.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate resampling with a stable model (Random Forest)\n",
    "resampling_models = {\n",
    "    'rf_oversampled': (RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1), X_train_over, y_train_over),\n",
    "    'rf_undersampled': (RandomForestClassifier(n_estimators=250, random_state=42, n_jobs=-1), X_train_under, y_train_under)\n",
    "}\n",
    "\n",
    "resample_results = []\n",
    "fitted_resampled = {}\n",
    "preds_resampled = {}\n",
    "\n",
    "for name, (model, X_tr, y_tr) in resampling_models.items():\n",
    "    fitted_pipe, y_pred, metrics = evaluate_pipeline(name, model, X_tr, y_tr, X_test, y_test)\n",
    "    resample_results.append(metrics)\n",
    "    fitted_resampled[name] = fitted_pipe\n",
    "    preds_resampled[name] = y_pred\n",
    "\n",
    "resample_df = pd.DataFrame(resample_results).sort_values('macro_f1', ascending=False)\n",
    "display(resample_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Global Comparison of All Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge all experiment tables\n",
    "all_results = pd.concat([\n",
    "    baseline_df,\n",
    "    weighted_df,\n",
    "    resample_df\n",
    "], ignore_index=True).sort_values('macro_f1', ascending=False)\n",
    "\n",
    "display(all_results)\n",
    "\n",
    "# Visual comparison on key imbalance-aware metrics\n",
    "plot_df = all_results[['model', 'macro_f1', 'balanced_accuracy']].set_index('model')\n",
    "plot_df.plot(kind='bar', figsize=(14, 6), colormap='tab20')\n",
    "plt.title('Model Comparison on Imbalance-Aware Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspect confusion matrix of the best model by macro_f1\n",
    "best_model_name = all_results.iloc[0]['model']\n",
    "\n",
    "if best_model_name in preds_baselines:\n",
    "    best_preds = preds_baselines[best_model_name]\n",
    "elif best_model_name in preds_weighted:\n",
    "    best_preds = preds_weighted[best_model_name]\n",
    "else:\n",
    "    best_preds = preds_resampled[best_model_name]\n",
    "\n",
    "labels = sorted(y.unique())\n",
    "cm = confusion_matrix(y_test, best_preds, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Best Model Confusion Matrix: {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Best model: {best_model_name}')\n",
    "print(classification_report(y_test, best_preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Tree-Based Best Model)\n",
    "\n",
    "If the best model is random-forest-based, we inspect the top transformed features.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 'rf' in best_model_name:\n",
    "    if best_model_name in fitted_baselines:\n",
    "        best_pipe = fitted_baselines[best_model_name]\n",
    "    elif best_model_name in fitted_weighted:\n",
    "        best_pipe = fitted_weighted[best_model_name]\n",
    "    else:\n",
    "        best_pipe = fitted_resampled[best_model_name]\n",
    "\n",
    "    preprocess_step = best_pipe.named_steps['preprocess']\n",
    "    model_step = best_pipe.named_steps['model']\n",
    "\n",
    "    feature_names = preprocess_step.get_feature_names_out()\n",
    "    importances = model_step.feature_importances_\n",
    "\n",
    "    fi = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    fi = fi.sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "    display(fi)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=fi, x='importance', y='feature', palette='crest')\n",
    "    plt.title('Top 20 Feature Importances (Best RF Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Best model is not random-forest-based; feature importances are skipped.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Comments and Conclusions\n",
    "\n",
    "Main takeaways from this analysis:\n",
    "- the target `sii` is strongly imbalanced (class 0 dominates)\n",
    "- high missingness affects several feature groups, so robust imputation is necessary\n",
    "- plain accuracy can be misleading on imbalanced data\n",
    "- metrics like `macro_f1` and `balanced_accuracy` provide a better comparison\n",
    "- random oversampling on train data can improve minority-class sensitivity compared to naive baselines\n",
    "\n",
    "This notebook is fully reproducible and can be extended with:\n",
    "- hyperparameter tuning (e.g., `GridSearchCV`)\n",
    "- more advanced resampling methods (SMOTE/ADASYN) if `imbalanced-learn` is installed\n",
    "- threshold tuning or cost-sensitive learning depending on project goals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}